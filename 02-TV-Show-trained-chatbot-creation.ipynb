{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65511f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Parse the SRT File\n",
    "# Step 2: Create Input-Response Pairs\n",
    "# Step 3: Train a Chatbot Model\n",
    "# Step 4: Example with HuggingFace GPT-2 (Fine-tuning)\n",
    "# Step 5: Test the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1869cd3c",
   "metadata": {},
   "source": [
    "# Step 1: Parse the SRT File\n",
    "Extract the dialogue only from the .srt file and discard timestamps and sequence numbers.\n",
    "\n",
    "We’ll:\n",
    "\n",
    "- Ignore numeric indices and timestamps\n",
    "\n",
    "- Keep only the spoken lines\n",
    "\n",
    "- Group alternating lines into a conversation (e.g., line 1 = user input, line 2 = response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_srt(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    blocks = re.split(r'\\n\\n+', text.strip())\n",
    "    dialogues = []\n",
    "\n",
    "    for block in blocks:\n",
    "        lines = block.strip().split('\\n')\n",
    "        if len(lines) >= 3:\n",
    "            spoken_lines = lines[2:]\n",
    "            dialogues.extend(spoken_lines)\n",
    "\n",
    "    return dialogues\n",
    "\n",
    "# Load and preview dialogues\n",
    "dialogues = parse_srt('/mnt/data/suits-1x01-pilot.en.srt')\n",
    "for line in dialogues[:10]:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a64ff",
   "metadata": {},
   "source": [
    "# Step 2: Create Input-Response Pairs\n",
    "To train a chatbot, we need conversation pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for i in range(len(dialogues)-1):\n",
    "    input_text = dialogues[i].strip()\n",
    "    response_text = dialogues[i+1].strip()\n",
    "    if input_text and response_text:\n",
    "        pairs.append((input_text, response_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ca094",
   "metadata": {},
   "source": [
    "# delete everything after here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb01c7",
   "metadata": {},
   "source": [
    "### What is a Natural Language Tool Kit?\n",
    "\n",
    "**Key Features of NLTK:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70822aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '!']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Tokenization – Breaking text into words or sentences\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a5529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rurig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rurig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'), ('runs', 'VBZ'), ('fast', 'RB'), ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Part-of-Speech (POS) Tagging – Labeling words with their grammatical roles\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import pos_tag\n",
    "pos_tag(word_tokenize(\"He runs fast.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ffcb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Stemming and Lemmatization – Reducing words to their root form\n",
    "from nltk.stem import PorterStemmer\n",
    "PorterStemmer().stem(\"running\")  # Output: 'run'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157df920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Named Entity Recognition (NER) – Identifying names of people, organizations, etc.\n",
    "# 5. Parsing and Syntax Trees – Analyzing grammatical structure of sentences\n",
    "# 6. Sentiment Analysis – Determining the emotion or opinion in text (with training)\n",
    "# 7. Stop Words Removal – Removing common words (like “the”, “is”, “in”)\n",
    "# 8. Text Classification and Machine Learning – Building simple NLP models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3d885",
   "metadata": {},
   "source": [
    "# Use Cases:\n",
    "- Chatbot development\n",
    "\n",
    "- Text summarization\n",
    "\n",
    "- Information extraction\n",
    "\n",
    "- Language translation\n",
    "\n",
    "- Spam filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a97ff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: pip, setuptools, wheel\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.3\n",
      "    Uninstalling pip-20.2.3:\n",
      "      Successfully uninstalled pip-20.2.3\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 50.3.0.post20201103\n",
      "    Uninstalling setuptools-50.3.0.post20201103:\n",
      "      Successfully uninstalled setuptools-50.3.0.post20201103\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.35.1\n",
      "    Uninstalling wheel-0.35.1:\n",
      "      Successfully uninstalled wheel-0.35.1\n",
      "Successfully installed pip-25.0.1 setuptools-75.3.2 wheel-0.45.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorboard 2.3.0 requires google-auth<2,>=1.6.3, but you'll have google-auth 2.40.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# ! pip install spacy\n",
    "! pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e7b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.2.tar.gz (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 878.0 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [113 lines of output]\n",
      "      Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "      Collecting setuptools\n",
      "        Downloading setuptools-75.3.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "      Collecting cython<3.0,>=0.25\n",
      "        Downloading Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Downloading cymem-2.0.11.tar.gz (10 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting preshed<3.1.0,>=3.0.2\n",
      "        Downloading preshed-3.0.10.tar.gz (15 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        × pip subprocess to install build dependencies did not run successfully.\n",
      "        │ exit code: 1\n",
      "        ╰─> [81 lines of output]\n",
      "            Collecting setuptools\n",
      "              Using cached setuptools-75.3.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "            Collecting cython>=0.28\n",
      "              Using cached cython-3.1.2-cp38-cp38-win_amd64.whl.metadata (5.8 kB)\n",
      "            Collecting cymem<2.1.0,>=2.0.2\n",
      "              Using cached cymem-2.0.11.tar.gz (10 kB)\n",
      "              Installing build dependencies: started\n",
      "              Installing build dependencies: finished with status 'done'\n",
      "              Getting requirements to build wheel: started\n",
      "              Getting requirements to build wheel: finished with status 'done'\n",
      "              Preparing metadata (pyproject.toml): started\n",
      "              Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "            Collecting murmurhash<1.1.0,>=0.28.0\n",
      "              Downloading murmurhash-1.0.13.tar.gz (13 kB)\n",
      "              Installing build dependencies: started\n",
      "              Installing build dependencies: finished with status 'done'\n",
      "              Getting requirements to build wheel: started\n",
      "              Getting requirements to build wheel: finished with status 'done'\n",
      "              Preparing metadata (pyproject.toml): started\n",
      "              Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "            Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "            Using cached cython-3.1.2-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "            Building wheels for collected packages: cymem, murmurhash\n",
      "              Building wheel for cymem (pyproject.toml): started\n",
      "              Building wheel for cymem (pyproject.toml): finished with status 'error'\n",
      "              error: subprocess-exited-with-error\n",
      "      \n",
      "              × Building wheel for cymem (pyproject.toml) did not run successfully.\n",
      "              │ exit code: 1\n",
      "              ╰─> [15 lines of output]\n",
      "                  running bdist_wheel\n",
      "                  running build\n",
      "                  running build_py\n",
      "                  creating build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  copying cymem\\about.py -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  copying cymem\\__init__.py -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  creating build\\lib.win-amd64-cpython-38\\cymem\\tests\n",
      "                  copying cymem\\tests\\test_import.py -> build\\lib.win-amd64-cpython-38\\cymem\\tests\n",
      "                  copying cymem\\tests\\__init__.py -> build\\lib.win-amd64-cpython-38\\cymem\\tests\n",
      "                  copying cymem\\cymem.pyx -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  copying cymem\\cymem.pxd -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  copying cymem\\__init__.pxd -> build\\lib.win-amd64-cpython-38\\cymem\n",
      "                  running build_ext\n",
      "                  building 'cymem.cymem' extension\n",
      "                  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "                  [end of output]\n",
      "      \n",
      "              note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "              ERROR: Failed building wheel for cymem\n",
      "              Building wheel for murmurhash (pyproject.toml): started\n",
      "              Building wheel for murmurhash (pyproject.toml): finished with status 'error'\n",
      "              error: subprocess-exited-with-error\n",
      "      \n",
      "              × Building wheel for murmurhash (pyproject.toml) did not run successfully.\n",
      "              │ exit code: 1\n",
      "              ╰─> [19 lines of output]\n",
      "                  running bdist_wheel\n",
      "                  running build\n",
      "                  running build_py\n",
      "                  creating build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  copying murmurhash\\about.py -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  copying murmurhash\\__init__.py -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  creating build\\lib.win-amd64-cpython-38\\murmurhash\\tests\n",
      "                  copying murmurhash\\tests\\test_hash.py -> build\\lib.win-amd64-cpython-38\\murmurhash\\tests\n",
      "                  copying murmurhash\\tests\\test_import.py -> build\\lib.win-amd64-cpython-38\\murmurhash\\tests\n",
      "                  copying murmurhash\\tests\\__init__.py -> build\\lib.win-amd64-cpython-38\\murmurhash\\tests\n",
      "                  copying murmurhash\\mrmr.pyx -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  copying murmurhash\\mrmr.pxd -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  copying murmurhash\\__init__.pxd -> build\\lib.win-amd64-cpython-38\\murmurhash\n",
      "                  creating build\\lib.win-amd64-cpython-38\\murmurhash\\include\\murmurhash\n",
      "                  copying murmurhash\\include\\murmurhash\\MurmurHash2.h -> build\\lib.win-amd64-cpython-38\\murmurhash\\include\\murmurhash\n",
      "                  copying murmurhash\\include\\murmurhash\\MurmurHash3.h -> build\\lib.win-amd64-cpython-38\\murmurhash\\include\\murmurhash\n",
      "                  running build_ext\n",
      "                  building 'murmurhash.mrmr' extension\n",
      "                  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "                  [end of output]\n",
      "      \n",
      "              note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "              ERROR: Failed building wheel for murmurhash\n",
      "            Failed to build cymem murmurhash\n",
      "            ERROR: Failed to build installable wheels for some pyproject.toml based projects (cymem, murmurhash)\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      error: subprocess-exited-with-error\n",
      "      \n",
      "      × pip subprocess to install build dependencies did not run successfully.\n",
      "      │ exit code: 1\n",
      "      ╰─> See above for output.\n",
      "      \n",
      "      note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# print(spacy.__version__)\n",
    "\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b5f0d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ffe058a5dbb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Apple is looking at buying a startup in the UK.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying a startup in the UK.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebc99e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
